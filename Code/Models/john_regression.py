import csv
import numpy as np
import math
import matplotlib.pyplot as plt
from scipy import polyfit, polyval, linalg
from error_score import *
import scipy.stats as stats

from import_data import import_data
from cross_validation import cross_validation

def main():
    wine_data, wine_features = import_data('winequality-red.csv')
    inputmtx = wine_data[:, 0:10]
    targets = wine_data[:, 11]
    train_inputmtx, train_targets, test_inputmtx, test_targets = cross_validation(inputmtx, targets, 0.25)
    
    train = wine_data[0:250,:]
    target = wine_data [250:1599,:]
    
    N = len(wine_data[:,11])
    print(N)
    
    folds = create_cv_folds(N, 2)
    
    print(folds)
    
#    k = 20   
#    rmse_list = []
#    
#    for k in range(1,40):
#        print(k)
#        kNN_targets = []
#        
#        for test_row in train:
#            euclidian_distances_per_test_row = []
#            print("BREAK")
#            for train_row in target:
#                euc_dis_and_target = []
#                euc_dis = euclidian_distance(test_row, train_row)
#                euc_dis_and_target.append(euc_dis)
#                euc_dis_and_target.append(train_row[11])
#                print(euc_dis_and_target)
#                euclidian_distances_per_test_row.append(euc_dis_and_target)
#                
#            #sort the list for that test_row
#            euc_dis_sorted = np.sort(euclidian_distances_per_test_row)
#            
#            #find k nearest neighbors from sorted list
#            kNN = euc_dis_sorted[0:k]
#            
#            
#    #        # find weighted sum of variable of interest (target)
#    #        weights = kNN[:,1] ** -1
#    #        print(weights)
#    #        weighted_targets = weights * kNN[:,0]
#    #        weighted_target = sum(weighted_targets)
#    #
#            # sum the target values and divide by k
#            weighted_target = (sum(kNN[:,0]))/k
#    #       print(weighted_target)
#            
#            kNN_targets.append(weighted_target)
#        
#    #    fig = plt.figure()
#    #    ax = fig.add_subplot(1,1,1)
#    #    ax.scatter(train[:,3],train[:,11])
#    #    ax.scatter(train[:,3],kNN_targets)
#    #    
#    #    plt.show()
#        rmse_error = []
#        rmse_error.append(rms_error(train[:,11], kNN_targets))
#        rmse_error.append(k)
#        rmse_list.append(rmse_error)
#    
#    rmse_array = np.array(rmse_list)
#    print(rmse_array)
#    print(type(rmse_array))
#    
#    fig = plt.figure()
#    ax = fig.add_subplot(1,1,1)
#    ax.plot(rmse_array[:,1],rmse_array[:,0], 'r')
#    ax.set_xlabel("k")
#    ax.set_ylabel("rmse")
#    plt.show()
#    
#    best_k = 0
#    min_rmse = min(rmse_array[:,0])
#    for row in rmse_array:
#        if(row[0] == min_rmse):
#            best_k = row[1]
#        
#        
#    print("min RMSE: ", min(rmse_array[:,0]))
#    print("best k: ", best_k)
#    
    
    
    # temporary min rsme (run in different file!)
#    arr = [(0.95897614483207816, 0.79424793581605879, 1.0, 13.515712268039056, 0.57651114590603048), (0.97552657705570622, 0.8086629879770646, 1.0, 13.931174415088201, 0.63553947069313654), (0.78735270042960048, 0.69544882716207301, 0.66666666666666696, 12.558236032319998, 0.55845461302136945), (0.72605345940484445, 0.63239688593937149, 0.5, 11.69945604195887, 0.51821783377656305), (0.76335965579807097, 0.67089474415882411, 0.59999999999999964, 12.26954639413278, 0.54909507886236519), (0.73019333837864653, 0.63507895346568699, 0.5, 11.774764449248712, 0.5243203731933207), (0.71391658310934947, 0.60409571248898219, 0.42857142857142883, 11.336520197498254, 0.50943397865241458), (0.70563355259584692, 0.58304886563791503, 0.375, 11.032992447568196, 0.49664216664523136), (0.70299666464038779, 0.56714636224124115, 0.33333333333333304, 10.806981821697558, 0.48824515731400364), (0.71773740245098205, 0.62185511060155885, 0.5, 11.526030864949167, 0.50905963791524345), (0.7053163879274178, 0.60441516908866699, 0.45454545454545414, 11.285831250375516, 0.4961427136980418), (0.70362512902133467, 0.56538135853181348, 0.33333333333333304, 10.799362272958449, 0.48844929637988821), (0.70797190132130228, 0.55862592340822381, 0.30769230769230749, 10.740145482654102, 0.4881626259748354), (0.70857967117970888, 0.55194502180410154, 0.28571428571428559, 10.641572174675119, 0.48454227015323054), (0.70975828664857799, 0.54618685322589122, 0.26666666666666661, 10.556883192636349, 0.48193432647075551), (0.6922896845380806, 0.57070198309597453, 0.375, 10.779064383635664, 0.47760240491986017), (0.69904395955153997, 0.5484253958550348, 0.29411764705882337, 10.5172410809748, 0.47476065546687268), (0.69657748105396045, 0.59206698142141212, 0.44444444444444464, 11.046842791938518, 0.48460258927902211), (0.69516447621275279, 0.59722247164133069, 0.47368421052631593, 11.082477373982057, 0.4807889790035087), (0.69694982923328708, 0.60552833989030042, 0.5, 11.173074830384973, 0.47968188805357082), (0.699990779739039, 0.61407384734239523, 0.52380952380952372, 11.27386835868043, 0.47949032370077455), (0.70334002935944728, 0.62085866503686482, 0.54545454545454586, 11.346726339589035, 0.47889330747310604), (0.69820335737321249, 0.61335495036847099, 0.52173913043478226, 11.253115407417244, 0.47655317472780295), (0.69538264240168923, 0.60503504548821052, 0.5, 11.150555881228327, 0.47745570470311871), (0.69232754682036524, 0.5990736436686821, 0.48000000000000043, 11.076118129741653, 0.47560783326842349), (0.69474122631012469, 0.60622404185005152, 0.5, 11.163612766768685, 0.47603168833486054), (0.69591447115131422, 0.6097598616275508, 0.51851851851851816, 11.193429977048188, 0.4749269319669982), (0.69861464568448139, 0.61546628817623217, 0.53571428571428559, 11.259189957999016, 0.47478098784257033), (0.71720363626878969, 0.64117247754877316, 0.60098220839827388, 11.545804394075514, 0.47476096735648798), (0.73048816604908318, 0.6551024136069874, 0.62103396026077773, 11.697616248331736, 0.473904155168485), (0.73568503197238655, 0.66019561441935948, 0.62940118668381562, 11.766695486956502, 0.47633411781728524), (0.74758645900537068, 0.66890810249303334, 0.66919601702882803, 11.839347456012458, 0.47393530224065011), (0.73808730652322763, 0.66125674920893418, 0.64112945313020964, 11.749641690495691, 0.47187027553848127), (0.7525127688670239, 0.67295715538794187, 0.67137954205841455, 11.875442495189054, 0.47147697727776727), (0.74384096075704276, 0.66670337015700643, 0.65219726942817458, 11.806935777732745, 0.46999298852719962), (0.73605870053179101, 0.66079701743890085, 0.63498214439456024, 11.741767610532358, 0.46864809594810403), (0.73933558466257199, 0.66361805018827658, 0.64397038999962453, 11.770521932280905, 0.46793786931291242), (0.74249870173572963, 0.66615441728858515, 0.65333959026279231, 11.795139633628459, 0.46710610220463983), (0.75504464938389482, 0.67559535620169964, 0.68404754606614748, 11.892489499854371, 0.46589530564151355), (0.74750632891668634, 0.67010547229665718, 0.66694635741449382, 11.831922906433935, 0.46517506311708506), (0.76003304860662413, 0.67898951040893507, 0.69462433206712593, 11.92325818935819, 0.46436647354160232), (0.75326681275232543, 0.6746242044646602, 0.67538306341382182, 11.884170076480576, 0.46561436717838889), (0.75526613901880157, 0.67595907442198, 0.68293229449722181, 11.894142835479075, 0.4643875855093979), (0.75817941875197326, 0.67884931213890476, 0.68624808363876788, 11.939582204032314, 0.46736728220952067), (0.76077992095662839, 0.68056833524533089, 0.69322034844679559, 11.954932218600947, 0.46665824660648147), (0.75386768396444692, 0.67572008734757028, 0.67815034087186499, 11.902871286413367, 0.46629816102070371), (0.7474148680936924, 0.67093093800661197, 0.66372161021501697, 11.849424097044235, 0.46560175933247572), (0.74057382973108621, 0.66543600354730692, 0.64989407666887056, 11.785462682831753, 0.46481560179661829), (0.73468104285480262, 0.66067976216366764, 0.63445520585658421, 11.730672143835276, 0.4639237952424029), (0.72869329701392604, 0.65520771573652437, 0.61844992729137749, 11.662420487236654, 0.46272288485610114), (0.73063296511467613, 0.65718353893324644, 0.62575667253820466, 11.684142508830817, 0.46206027446617759), (0.73424794671315685, 0.66052855996923476, 0.63295365960477756, 11.731997809052064, 0.46423727589316666), (0.72991048244156609, 0.65722691764573304, 0.62101113772544236, 11.701130594561533, 0.46490899788995166), (0.72475737424496178, 0.65289925491424938, 0.6059489732718677, 11.652821410592292, 0.46431528543233913), (0.72052806711411044, 0.64900243595231832, 0.59543808622128358, 11.607063268827289, 0.46361053852896855), (0.72311267304598559, 0.65111614390992756, 0.60120669848669905, 11.634216359443693, 0.46492070070273916), (0.71924619886193164, 0.64751225205509677, 0.59065921254833587, 11.592281705664844, 0.46432680430766776), (0.72190507421981764, 0.64995806996992855, 0.59615873012776799, 11.625882672587981, 0.4659217488234571), (0.72330729703295127, 0.65081658882162319, 0.60084491282756813, 11.630882870974926, 0.46581997071586723), (0.72461348269024373, 0.65162521310079946, 0.60749749761377547, 11.636056169225645, 0.4657875903393508), (0.72701949342763095, 0.65352266996108832, 0.61393196486600887, 11.660545960012389, 0.46705024206008056), (0.7284151798344991, 0.65469101569748567, 0.61845756644261796, 11.669255730543762, 0.46618026609508484), (0.72965104339536135, 0.65564629430096599, 0.62350436617753324, 11.674762171188322, 0.46513903424135145), (0.72582463153279386, 0.65231364463501795, 0.61475576749128624, 11.636190816743154, 0.46453922505136142), (0.72232462910861539, 0.64919748415778011, 0.60529798645295863, 11.600454240201634, 0.46405049778569901), (0.7237388935196285, 0.65042875317297999, 0.61031477377552434, 11.610936230812978, 0.46349306237482885), (0.72049849088996853, 0.64752683148383106, 0.60120559804753126, 11.578025280950158, 0.46311407480871514), (0.72232825250768162, 0.64931262359051189, 0.60707022160565582, 11.597492718926267, 0.46291833147768024), (0.72944490180580224, 0.65588882769479628, 0.61883491735762286, 11.672454225846655, 0.46325338094577762), (0.73127540328872276, 0.65750414008112956, 0.62307370694581454, 11.689866536468555, 0.46307466493629262), (0.73864529848401572, 0.66377972552767894, 0.64246703501699987, 11.760851182391148, 0.46343796689207672), (0.73485131879017007, 0.66072356234991791, 0.63354388175287468, 11.726053163624002, 0.46296638164657578), (0.73142875694266263, 0.65822413678244185, 0.6241176831703994, 11.704428872563803, 0.46365942298946061), (0.73314941345193296, 0.65972453310359058, 0.62919717393836683, 11.720594430077877, 0.4634855601854298), (0.73480366568699207, 0.66106153932887579, 0.63414121161918846, 11.733832701100534, 0.46316135133603425), (0.73644475364697048, 0.66236336117981187, 0.63895514304525181, 11.746705714575798, 0.46284738001297998), (0.74375161894777997, 0.66813941670909216, 0.65663105027843027, 11.81470118389529, 0.46422895589685265), (0.74013031995711764, 0.66527350454063205, 0.64821270347998894, 11.782512642826886, 0.46397605971391115), (0.74216562321710622, 0.66679921859690494, 0.65266570723340678, 11.801331930729054, 0.46457016160719872), (0.73909572042806226, 0.66454439493488393, 0.64518949357758792, 11.779758984447685, 0.46494735112358321), (0.74005407216218577, 0.6653821484718726, 0.64335822225939276, 11.787616471130407, 0.46457760889527949), (0.74104322504386444, 0.66619946899576421, 0.64192714268246354, 11.795249245137869, 0.46428022084473886), (0.74216183908011779, 0.66692152540831517, 0.64478834473130942, 11.799904946298891, 0.46365818173772905), (0.73843708912570216, 0.66409120583680625, 0.63762127420477999, 11.767434065229647, 0.46281648495502931), (0.74458316937801594, 0.66915198287671385, 0.65098041029231801, 11.825970086038746, 0.46346725509029441), (0.75168881463132753, 0.67446516900683218, 0.66642207764832939, 11.89013060072355, 0.46529269302754361), (0.75386026310161081, 0.67634797101012667, 0.67025630664087732, 11.918801926962926, 0.46710538637792232), (0.75051217688579841, 0.67394185807744578, 0.66263975770177641, 11.892906746546196, 0.46691633296686008), (0.75188320156358945, 0.67490880349230586, 0.66643032222198118, 11.901899943989584, 0.46660690704801094), (0.75903113537451394, 0.68011066672949116, 0.68124776308618129, 11.964553965587678, 0.46859025939056337), (0.76078145312522649, 0.68122987595996964, 0.68475053492039972, 11.978769574438454, 0.46932473905553923), (0.7628869880963145, 0.68292918780673995, 0.68817715954082992, 12.004477531836148, 0.47103104086193526), (0.76444536675233743, 0.68412272176528255, 0.69153009330920812, 12.020755646792912, 0.47190418643056758), (0.76599515397803086, 0.68529086138428152, 0.69481168806123783, 12.03674653170939, 0.47279124885490192), (0.76729028607301997, 0.6861073946010835, 0.69802419660796167, 12.044417850391726, 0.47265282757858673), (0.7638838518679858, 0.68371044257398894, 0.69075311122662875, 12.018011994618766, 0.47230180418168577), (0.76991472433768338, 0.68809135656670828, 0.70425050183253957, 12.066783204642086, 0.47282312145551014), (0.77123495460072333, 0.68896913678918692, 0.70726835385465714, 12.076106140530145, 0.47281675072891349), (0.77169831766503849, 0.68920036976933086, 0.7065041547327171, 12.075476360696511, 0.47191858123541897)]
#    rsme = []
#    for i in range(0,99):
#        arr2 = arr[i]
#        rsme.append(arr2[0])   
#    print(rsme)
#    print(min(rsme))
    
    
    
    
#    euclidian_distances_array = np.array(euclidian_distances)
#    print(len(euclidian_distances_array))
#    euclidian_distances_sorted = np.sort(euclidian_distances_array)
#    print(euclidian_distances_sorted[1:200])
    
    
        
    
def euclidian_distance(vec1, vec2):
    joined_vectors = zip(vec1, vec2)
    distance = 0;
    for element in joined_vectors:
        distance += (element[1] - element[0]) ** 2
    
    return math.sqrt(distance)
        
def construct_knn_approx(train_inputs, train_targets, k):  
    """
    For 1 dimensional training data, it produces a function:reals-> reals
    that outputs the mean training value in the k-Neighbourhood of any input.
    """
    # Create Euclidean distance.
    distance = lambda x,y: (x-y)**2
    train_inputs = np.resize(train_inputs, (1,train_inputs.size))
    def prediction_function(inputs):
        inputs = inputs.reshape((inputs.size,1))
        distances = distance(train_inputs, inputs)
        predicts = np.empty(inputs.size)
        for i, neighbourhood in enumerate(np.argpartition(distances, k)[:,:k]):
            # the neighbourhood is the indices of the closest inputs to xs[i]
            # the prediction is the mean of the targets for this neighbourhood
            predicts[i] = np.mean(train_targets[neighbourhood])
        return predicts
    # We return a handle to the locally defined function
    return prediction_function
    

def create_cv_folds(N, num_folds):
    """
    Defines the cross-validation splits for N data-points into num_folds folds.
    Returns a list of folds, where each fold is a train-test split of the data.
    Achieves this by partitioning the data into num_folds (almost) equal
    subsets, where in the ith fold, the ith subset will be assigned to testing,
    with the remaining subsets assigned to training.

    parameters
    ----------
    N - the number of datapoints
    num_folds - the number of folds

    returns
    -------
    folds - a sequence of num_folds folds, each fold is a train and test array
        indicating (with a boolean array) whether a datapoint belongs to the
        training or testing part of the fold.
        Each fold is a (train_part, test_part) pair where:

        train_part - a boolean vector of length N, where if ith element is
            True if the ith data-point belongs to the training set, and False if
            otherwise.
        test_part - a boolean vector of length N, where if ith element is
            True if the ith data-point belongs to the testing set, and False if
            otherwise.
    """
    # if the number of datapoints is not divisible by folds then some parts
    # will be larger than others (by 1 data-point). min_part is the smallest
    # size of a part (uses integer division operator //)
    min_part = N//num_folds
    # rem is the number of parts that will be 1 larger
    rem = N % num_folds
    # create an empty array which will specify which part a datapoint belongs to 
    parts = np.empty(N, dtype=int)
    start = 0
    for part_id in range(num_folds):
        # calculate size of the part
        n_part = min_part
        if part_id < rem:
            n_part += 1
        # now assign the part id to a block of the parts array
        parts[start:start+n_part] = part_id*np.ones(n_part)
        start += n_part
    # now randomly reorder the parts array (so that each datapoint is assigned
    # a random part.
    np.random.shuffle(parts)
    # we now want to turn the parts array, into a sequence of train-test folds
    folds = []
    for f in range(num_folds):
        train = (parts != f)
        test = (parts == f)
        folds.append((train,test))
    return folds
    
    
    

if __name__ == '__main__':

  main()